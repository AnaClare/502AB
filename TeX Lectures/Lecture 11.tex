\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath,mathtools,graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\title{Math 502AB - Lecture 11}
\author{Dr. Jamshidian}
\date{October 2, 2017}
\begin{document}

\maketitle

\section{Lecture - Part 1}

\subsection{Chapter 4: Jointly Distributed Random Variables}

To address the relationships between multiple random variables, we need \textbf{joint distributions} of random variables

\subsubsection*{Definition:}
    An $n$-dimensional random vector $\overline{X} = (x_1,...,x_n)$ where each component is a random variable.

\subsubsection*{Example}

A fair coin is tossed 3 times. Let $X$ be the number of heads on the $1^{st}$ toss and $Y$ be the total number of heads.

\begin{equation*}
    \begin{split}
        X &= \text{ Number of Heads on First Toss}\\
        Y &= \text{ Total Number of Heads}
    \end{split}
\end{equation*}

With the probability matrix:

\begin{equation*}
    \begin{bmatrix}
        x/y & 0 & 1 & 2 & 3\\
        0 & \frac{1}{8} & \frac{2}{8} & \frac{1}{8} & 0\\
        1 & 0 & \frac{1}{8} & \frac{2}{8} & \frac{1}{8}
    \end{bmatrix}
\end{equation*}

Calculating joint probabilities:
\begin{equation*}
\begin{split}
    P(X = 0 \cap Y = 0) &= P(Y=0 | X=0) \cdot P(X=0)\\
        &= \frac{1}{4} \cdot \frac{1}{2} = \frac{1}{8}
\end{split}
\end{equation*}

You can also use the probability matrix to find values of the \textbf{marginal density} by summing over the rows/column probabilities of the other variable. For example:

\begin{equation*}
\begin{split}
    P(Y=2) &= \frac{1}{8} + \frac{2}{8} = \frac{3}{8}\\
    P(X=1) &= 0 + \frac{1}{8} + \frac{2}{8} + \frac{1}{8} = \frac{1}{2}
\end{split}
\end{equation*}

\subsubsection{Joint Discrete Random Variables}

\subsubsection*{Definition:}

Let $(X,Y)$ be a bivariate random variable. Then, the function $f(x,y) = P(X=x,Y=y)$, $\forall x,y$ is called a \textbf{joint pmf} of $(X,Y)$.

\noindent\textbf{Notation:} Sometimes, $f_{x,y}(x,y)$ is used to indicate the joint distribution

\subsubsection*{Example:}

A bin of five transistors contains two defective units. The transistors are to be continually tested until the two defective units are discovered. Let:

\begin{equation*}
    \begin{split}
        X &= \text{ Number of tests until the first defective is discovered}\\
        Y &= \text{ Number of add'l tests until the second defective is discovered}
    \end{split}
\end{equation*}

Obtain the joint \textit{pmf} of $(X,Y)$

\begin{equation*}
    \begin{bmatrix}
        X/Y & 1 & 2 & 3 & 4\\
        1 & \left(\frac{2}{5}\right)\left(\frac{1}{4}\right) & \left(\frac{2}{5}\right)\left(\frac{1}{4}\right) & \left(\frac{2}{5}\right)\left(\frac{1}{4}\right) & \left(\frac{2}{5}\right)\left(\frac{1}{4}\right)\\
        2 & \left(\frac{3}{5}\right)\left(\frac{2}{4}\right)\left(\frac{1}{3}\right) & \left(\frac{3}{5}\right)\left(\frac{2}{4}\right)\left(\frac{1}{3}\right) & \left(\frac{3}{5}\right)\left(\frac{2}{4}\right)\left(\frac{1}{3}\right) & 0\\
        3 & \left(\frac{3}{5}\right)\left(\frac{2}{4}\right)\left(\frac{2}{3}\right)\left(\frac{1}{2}\right) & \left(\frac{3}{5}\right)\left(\frac{2}{4}\right)\left(\frac{2}{3}\right)\left(\frac{1}{2}\right) & 0 & 0\\
        4 & \left(\frac{3}{5}\right)\left(\frac{2}{4}\right)\left(\frac{1}{3}\right)(1) & 0 & 0 & 0
    \end{bmatrix}
\end{equation*}

\subsubsection*{Examples:}
\begin{enumerate}
    \item On average, what is the total number of tries to discover both defects? In other words, we wish to find $E[X + Y]$. First, we note that:

    \begin{equation*}
        E[g(x,y)] = \sum_{\text{all }(x,y)} g(x,y)f_{x,y}(x,y)
    \end{equation*}

    Thus, we have:
    \begin{equation*}
        \begin{split}
            E[X+Y] &= \sum_{x=1}^4\sum_{y=1}^4 (x+y)f(x,y)\\
                    &= \frac{1}{10} (2 + 3 + 4 + 5 + 3 + 4 + 5 + 4 + 5 + 5)\\
                    &= \frac{1}{10} (40) = 4
        \end{split}
    \end{equation*}

    \item What is the probability that the number of additional tests made is equal to 2?

    \begin{equation*}
        P(Y=2) = \frac{1}{10} + \frac{1}{10} + \frac{1}{10} = \frac{3}{10}
    \end{equation*}
\end{enumerate}

\subsubsection*{Definition:}

The \textit{marginal distribution} of $Y$ is:

\begin{equation*}
    f_Y(y) = \sum_{\text{all } X} P(X = x, Y=y) = \sum_{\text{all } X} f_{x,y}(x,y)
\end{equation*}

\subsubsection*{Example:}
Find the marginal distribution of $X$ from the prior example:

\begin{equation*}
    \begin{bmatrix}
        X & 1 & 2 & 3 & 4\\
         & \frac{4}{10} & \frac{3}{10} & \frac{2}{10} & \frac{1}{10}
    \end{bmatrix}
\end{equation*}

\subsubsection{Joint Continuous Random Variables}

\subsubsection*{Definition:}

A function $f(x,y)$ is called the \textit{joint pdf} of a continuous random variable $(x,y)$ if $\forall A \subset \mathcal{R}^2$

\begin{equation*}
    P[(x,y) \in A] = \int_A \int f(x,y) dx dy
\end{equation*}

We then have:
\begin{equation*}
    \begin{split}
        E[g(X,Y)] &= \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y) f(x,y) dx dy\\
        f_X(x) &= \int_{-\infty}^\infty f(x,y)dy \quad f_Y(y) = \int_{-\infty}^\infty f(x,y) dx
    \end{split}
\end{equation*}

\noindent \textbf{Properties:}
\begin{equation*}
    \begin{split}
        f(x,y) \geq 0 &\quad \forall (x,y)\\
        \int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y)dxdy &= 1
    \end{split}
\end{equation*}

\subsubsection*{Example:}

\begin{enumerate}
    \item Consider $(X,Y)$ with joint pdf:

    \begin{equation*}
        f(x,y) = \begin{cases}
            4x^2y + 2y^5 & 0 \leq x \leq 1; 0 \leq y \leq 1\\
            0 & \text{ otherwise}
        \end{cases}
    \end{equation*}

    \begin{enumerate}
        \item Compute $P(X+Y \geq 1)$

        When doing these kinds of problems, don't focus on the density so much as the \textbf{domain} where the density is positive.
        \begin{equation*}
            P(X+Y\geq 1) = \int_0^1 \int_{1-x}^1 (4x^2y+2y^5) dy dx
        \end{equation*}

        \item Compute $E[X+Y]$

        \begin{equation*}
            \int_0^1 \int_0^1 (x+y) (4x^2y+2y^5) dy dx
        \end{equation*}

        \item Find the \textit{marginal distribution} of $X$

        \begin{equation*}
            f_X(x) = \int_0^1 (4x^2y+2y^5)dy
        \end{equation*}
    \end{enumerate}

    \item Consider $(X,Y)$ with joint pdf:

    \begin{equation*}
        f(x,y) = \begin{cases}
            \lambda^2 e^{-\lambda y} & 0 \leq x \leq y\\
            0 & \text{ otherwise}
        \end{cases}
    \end{equation*}

    \begin{enumerate}
        \item Obtain the marginal distribution of $Y$ and $X$

        \begin{equation*}
            \begin{split}
                f_Y(y) &= \int_0^y \lambda^2 e^{-\lambda y}dx = \lambda^2 y e^{-\lambda y} \quad y \geq 0\\
                f_X(x) &=\int_0^\infty \lambda^2 e^{-\lambda y}dy = \lambda e^{-\lambda x} \quad x \geq 0
            \end{split}
        \end{equation*}

        \item Find $P(X+Y \leq 1)$

        Remember, for this problem it's important to think about the \textit{domain} of the functions.

        \begin{equation*}
            \int_0^{0.5} \int_x^{1-x} \lambda^2 e^{-\lambda y} dy dx
        \end{equation*}
    \end{enumerate}
\end{enumerate}

\subsubsection{Cumulative Distribution Function for (X,Y)}

We want to find:
\begin{equation*}
    F_{X,Y}(x,y) = P(X\leq x, Y \leq y)
\end{equation*}

\noindent \textbf{In the continuous case:}

\begin{equation*}
    \int_{-\infty}^x \int_{-\infty}^y f_{x,y}(s,t)dt ds
\end{equation*}

By the Fundamental Theorem of Calculus, we then have:
\begin{equation*}
    \frac{\partial^2}{\partial x \partial y} F(x,y) = f_{x,y}(x,y)
\end{equation*}

\subsubsection{Conditional Distribution of (X,Y)}

If $X$ and $Y$ are \textbf{discrete random variables}, then the \textit{conditional pmf} of $X|Y = y$ is:

\begin{equation*}
    \begin{split}
        f_{X|Y}(x|y) &= P(X=x|Y=y)\\
            &= \frac{P(X=x,Y=y)}{P(Y=y)} = \frac{f_{x,y}(x,y)}{f_Y(y)}
    \end{split}
\end{equation*}

Similarly, for the \textbf{continuous case}:

\begin{equation*}
    f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} \quad \forall f_Y(y) > 0
\end{equation*}

\section{Lecture - Part 2}

\subsubsection*{Example:}
\begin{enumerate}
    \item Recall the example of the failed parts from earlier, with joint pmf:

    \begin{equation*}
        \begin{bmatrix}
            X/Y & 1 & 2 & 3 & 4\\
            1 & \frac{1}{10} & \frac{1}{10} & \frac{1}{10} & \frac{1}{10}\\
            2 &  \frac{1}{10} & \frac{1}{10} & \frac{1}{10} & 0\\
            3 & \frac{1}{10} & \frac{1}{10} & 0 & 0\\
            4  & \frac{1}{10} & 0 & 0 & 0
        \end{bmatrix}
    \end{equation*}

    \begin{equation*}
        \begin{split}
            f_{Y|X}(y|1) &= \frac{f_{X,Y}(1,y)}{f_X(1)} = \frac{1/10}{4/10} = \frac{1}{4}, \quad y = 1,2,3,4\\
            f_{Y|X}(y|2) &= \frac{f_{X,Y}(2,y)}{f_X(2)} = \frac{1/10}{3/10} = \frac{1}{3}, \quad y = 1,2,3\\
            f_{Y|X}(y|3) &= \frac{1}{2}, \quad y = 1,2\\
            f_{Y|X}(y|4) &= 1, \quad y = 1
        \end{split}
    \end{equation*}

    \item Consider the joint density:
    \begin{equation*}
        f_{X,Y}(x,y) = 6 e^{-2x}e^{-3y} \quad x>0, y>0
    \end{equation*}

    Obtain the conditional density $f_{X|Y}(x|y)$.

    \begin{enumerate}
        \item First, we do it the hard way:
        \begin{equation*}
            f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}
        \end{equation*}

        To do this, we need to calculate the \textit{marginal distribution of Y}

        \begin{equation*}
        \begin{split}
            f_Y(y) = \int_0^\infty 6e^{-2x}e^{-3y} dx &= 6e^{-3y}\int_0^\infty e^{-2x}dx\\
            &= 3e^{-3y} \quad y>0
        \end{split}
        \end{equation*}

        But notice that in the conditional probability, it will be a function of $X$ only. So, in other words, by using the kernel we know that:

        \begin{equation*}
            \begin{split}
                f_{X|Y} (x|y) &\propto f_{X,Y}(x,y)\\
                &\propto 6e^{-2x}e^{-3y}\\
                \Rightarrow f_{X|Y}(x|y) &= 2e^{-2x}\quad x>0
            \end{split}
        \end{equation*}
    \end{enumerate}

    \item Consider the function $f_{X,Y}(x,y) = 24xy$ with $0<X<1$, $0<Y<1$, and $0<X+Y<1$. Obtain $f_{X|Y}(x|y)$.

    \begin{equation*}
        \begin{split}
            f_Y(y) &= \int_0^{1-y} 24xy dx = 12y(1-y)^2\\
            f_{X|Y}(x|y) &= \frac{24xy}{12y(1-y)^2} = \frac{2x}{(1-y)^2} \quad 0 \leq X \leq 1-Y
        \end{split}
    \end{equation*}

    \textbf{Alternate Solution:}

    \begin{equation*}
        \begin{split}
            f_{X|Y}(x|y) &\propto f_{X,Y}(x,y)\\
                    &\propto x
        \end{split}
    \end{equation*}

    We can rewrite the joint distribution as:

    \begin{equation*}
        \begin{split}
            f_{X,Y}(x,y) &= 24xy \cdot \mathcal{I}_{\{0<x<1\}}\cdot \mathcal{I}_{\{0<y<1\}}\cdot \mathcal{I}_{\{0<x+y<1\}}\\
            \Rightarrow f_{X|Y}(x|y) &\propto x \cdot \mathcal{I}_{\{0<x<1\}}\cdot \mathcal{I}_{\{-1\leq x<1-y\}}\\
            &= x \cdot \mathcal{I}_{\{0<x<1-y\}}
        \end{split}
    \end{equation*}

    So now we have:
    \begin{equation*}
        \int_0^{1-y}x dx = \frac{x^2}{2}\Biggr|^{1-y}_0 = \frac{(1-y)^2}{2}
    \end{equation*}

    And you can combine it with the \textbf{kernel} and you are done.
\end{enumerate}

\subsubsection{Conditional Expectation}

We now have:
\begin{equation*}
    \begin{split}
        E[X|Y=y] &= \int_{-\infty}^\infty x f_{X|Y}(x|y)dx\\
        Var(X|Y=y) &= E[X^2 | Y=y] + \left(E[X|Y=y] \right)^2
    \end{split}
\end{equation*}

\subsubsection*{Example:}

Considering the above example, find the \textit{conditional variance}.

\begin{equation*}
    \begin{split}
        E[X|Y=y] &= \int_0^{1-y} x \cdot \frac{2x}{(1-y)^2}dx = \frac{2}{3}(1-y)\\
        E[X^2|Y=y] &= \int_0^{1-y} x^2 \cdot \frac{2x}{(1-y)^2}dx = \frac{1}{2}(1-y)^2\\
        Var(X|Y=y) &= \frac{1}{2}(1-y)^2 - \left[\frac{2}{3}(1-y)\right]^2 = \frac{1}{18}(1-y)^2
    \end{split}
\end{equation*}

\subsection{Independent Random Variables}

\subsubsection*{Definition: Independent R.V.}

Let $X,Y$ have a joint pdf (or pmf) $f(x,y)$. We say that $X$ is independent of $Y$ if:
\begin{equation*}
    f(x,y) = f_X(x) f_Y(y) \quad \forall X,Y
\end{equation*}

\subsubsection*{Notation:}
If $X$ and $Y$ are independent, we notate it as:
\begin{equation*}
    X \indep Y
\end{equation*}

It can be shown that if $X \indep Y$, then
\begin{equation*}
    f_{X|Y}(x|y) = f_X(x) \quad \forall x,y
\end{equation*}

\subsubsection*{Theorem:}
Let $(X,Y)$ have a joint \textit{CDF} F(x,y) with respective marginal CDF's $F_X$ and $F_Y$. Then
\begin{equation*}
    X \indep Y \iff F_{X,Y}(x,y) = F_X(x) F_Y(y) \quad \forall x,y
\end{equation*}

\subsubsection*{Proof: (Continuous)}

($\Leftarrow$)

\begin{equation*}
\begin{split}
    F_{X,Y}(x,y) &= F_X(x) F_Y(y)\\
    \Rightarrow \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y) &= \frac{\partial^2}{\partial x \partial y} F_X(x) F_Y(y)\\
    \Rightarrow f_{X,Y}(x,y) &= \frac{\partial}{\partial x} F_X(x) \frac{\partial}{\partial y} F_Y(y)\\
    f_{X,Y}(x,y) &= f_X(x) f_Y(y)
\end{split}
\end{equation*}

\noindent ($\Rightarrow$) Suppose $X \indep Y$, then $f_{X,Y} (x,y) = f_X(x) f_Y(y)$

\begin{equation*}
    \begin{split}
        F_{X,Y} &= \int_{-\infty}^x \int_{-\infty}^y f_{X,Y}(s,t)dt ds\\
            &= \int_{-\infty}^x \int_{-\infty}^y f_X(s) f_Y(t) dt ds\\
            &= \int_{-\infty}^x f_X(s) ds \int_{-\infty}^y f_Y(t) dt = F_X(x) F_Y(y)
    \end{split}
\end{equation*}

\subsubsection*{Theorem:}

Let $(X,Y)$ have a joint pdf $f(x,y)$. Then $X \indep Y$ \textbf{iff} $\exists$ functions $g(x)$ and $h(y)$ such that $f(x,y)=g(x)h(y)$ $\forall x,y$

\subsubsection*{Proof:}

($\Rightarrow$): Suppose $X \indep Y$, thus let $g(x) = f_X(x)$ and $h(y) = f_Y(y)$.

\noindent ($\Leftarrow$): We need to show that if $f_{X,Y}(x,y) = g(x)h(y)$ for some $g$ and $h$, then
\begin{equation*}
    f_{X,Y}(x,y) = f_X(x) f_Y(y)
\end{equation*}

Define:

\begin{equation*}
    c = \int_{-\infty}^\infty g(x)dx \text{ and } d = \int_{-\infty}^\infty h(y) dy
\end{equation*}

Then:

\begin{equation*}
    \begin{split}
        cd &= \int_{-\infty}^\infty \int_{-\infty}^\infty g(x) h(y) dx dy\\
        &= \int_{-\infty}^\infty \int_{-\infty}^\infty f_{X,Y}(x,y) dx dy = 1
    \end{split}
\end{equation*}

On the other hand:

\begin{equation*}
    \begin{split}
        f_X(x) &= \int_{-\infty}^\infty g(x)h(y)dy = g(x) \int_{-\infty}^\infty h(y) dy\\
        f_Y(y) &= \int_{-\infty}^\infty g(x) h(y) dx = h(y) \int_{-\infty}^\infty g(x) dx
    \end{split}
\end{equation*}

We note that the values of these two integrals are $d$ and $c$, respectively. Then:

\begin{equation*}
    \begin{split}
        f_{X,Y}(x,y) = g(x) h(x) &= c\cdot d \cdot g(x) \cdot h(y)\\
        &= f_Y(y) f_X(x)
    \end{split}
\end{equation*}

\subsubsection*{Examples:}
\begin{enumerate}
    \item Consider the joint density
    \begin{equation*}
        f(x,y) = 6 e^{-2x} e^{-3y} \quad x>0; y>0
    \end{equation*}
    Are $X$ and $Y$ independent? \textbf{Yes.} There are two separate functions here like we want!

    \item Consider the joint density:

    \begin{equation*}
        f(x,y) = \begin{cases}
            24xy & 0<x<1; 0<y<1; 0 < x+1<1\\
            0 & \text{ otherwise}
        \end{cases}
    \end{equation*}

    Are $X$ and $Y$ independent? \textbf{No.} There is an indicator function which is dependent on \textit{both} $X$ and $Y$

\end{enumerate}



\end{document}
