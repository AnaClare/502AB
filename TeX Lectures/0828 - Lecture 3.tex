\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath}

\title{Math 502AB - Lecture 3}
\author{Dr. Jamshidian}
\date{August 28, 2017}
\begin{document}

\maketitle

\section{Homework Review}

\textbf{Question 42:} This is an inclusion/exclusion problem. Mori suggested when he assigned the homework to prove this problem by induction. 

The inclusion exclusion is essentially

\begin{equation*}
    P\left(\bigcup\limits_{i=1}^n A_i\right) = \sum\limits_{i=1}^n P(A_i) - \sum\limits_{i=1}^n P(A_i \cap A_j) + ... + (-1)^k \sum\limits_{1\leq i_1 \leq i_2 \leq i_n \leq n} P\left(A_{i1} \cap A_{i2} \cap ... \cap A_{ik}\right)
\end{equation*}

The point is that we have to prove that if it is an element of the set, then we will count it once.

We will define $E_1$ to be the set of simple points in $A_1, A_2,...,A_n$ only. What this means is that we are considering \textit{only} points in a given $A_i$ but not in any intersection $A_i \cap A_j$ for $i\neq j$. We will define $E_2$ to be the set of simple points that are in \textit{pairs} of sets $A_i \cap A_j$ for $i,j = 1,...,n$. 

In general we will say $E_k$ is the set of simple points in the intersection of $k$-sets only $A_{i1} \cap ... \cap A_{ik}$. And $E_n$ will be the set of points in $A_1 \cap ... \cap A_n$.

\noindent\textbf{A few things:}
\begin{enumerate}
    \item The way we have constructed our $E_i$ sets makes them \textbf{disjoint}.
    \item If we take the union of all of the $E_i$ sets, it is equivalent to the intersection of all $A_i$
    \begin{equation*}
        \bigcup\limits_{i=1}^n E_i = \bigcup\limits_{i=1}^n A_i
    \end{equation*}
    
    \item By finite additivity, we have:
     \begin{equation*}
        P\left(\bigcup\limits_{i=1}^n A_i\right) = P\left(\bigcup\limits_{i=1}^n E_i \right) = \sum\limits_{i=1}^n P(E_i)
    \end{equation*}
    \end{enumerate}
    So, we consider:
    \begin{equation*}
    \begin{split}
         P\left(\bigcup\limits_{i=1}^n A_i\right) = p_1 - p_2 + ... \pm p_n
    \end{split}
    \end{equation*}
    
    Where $p_1 = \sum\limits_{i=1}^n P(A_i)$, $\quad p_2 = \sum\limits_{i=1}^n P(A_{ij} \cap A_{ik})$, etc.
    
    The idea is that we want to make sure that if we have an element somewhere in this union, that it is only counted once. So, WLOG, suppose that $x \in E_k$, for some $k$. What we want to show is that when we use the above formulas, this $x$ is only counted once. 
    
    If this $x$ is in the intersection of all of the sets $A_i$ for $i = 1,...,k$, then $x$ is counted $k \choose 1$ times. In the $p_2$ sum, $x$ is counted $k \choose 2$ times (since there are $k \choose 2$ unique pairs of set intersections $A_i \cap A_k$), and so on.
    
    Thus, the total number of times that $x$ is counted is:
    \begin{equation*}
        {k \choose 1} - {k \choose 2} + {k \choose 3} + ... \pm {k \choose k} = 1
    \end{equation*}
    
    And you are done.
    
    \\~\\
    
    \noindent \textbf{Question 26:} To solve this problem, there are two ways to look at it. First, if it must be cast more than 5 times, then we failed the first 5 times. Thus, since our failure rate is $\frac{5}{6}$, then the probability is just $\left(\frac{5}{6}\right)^5$. 
    
    \\~\\
    
    \noindent \textbf{Alternatively}, if $x$ is the number of trials until the $1^{st}$ success, then:
    \begin{equation*}
        x \sim geometric \left(\frac{1}{6}\right)
    \end{equation*}
    
    And we have:
    \begin{equation*}
    \begin{split}
        P(X=k) &= (1-p)^{k-1}p \quad , k = 1,2,...\\
       \Rightarrow P(X\geq 6) &= \sum\limits_{k=6}^\infty \left(\frac{5}{6}\right)^{k-1}\left(\frac{1}{6}\right)
    \end{split}
    \end{equation*}
    
    which you can solve accordingly.
    
    \\~\\
    
    \noindent \textbf{Question 27b:} This is really just an expansion type of problem.
    
    \begin{equation*}
    \begin{split}
        \sum \limits_{k=1}^n k {n\choose k} &= n \cdot  2^{n-1}\\
        \sum \limits_{k=1}^n k \frac{n!}{k!(n-k)!} &= n \sum \limits_{k=1}^n k \frac{(n-1)!}{(k-1)!(n-k)!}\\
        n \sum \limits_{k=1}^n {{n-1}\choose{k-1}} &= n \sum \limits_{k=1}^{n-1} {{n-1}\choose{k}} 1^k \cdot 1^{n-k-1}\\
        &= n(1+1)^{n-1}
    \end{split}
    \end{equation*}

\noindent \textbf{Question 24b:} One way is to use recursive methods (watch office hours for this method). The idea is that if Ann is starting, what is the probability that we get heads on an odd flip? This can be sort of treated as a geometric problem.

Let $E_i$ be the event that $H$ occurs on the $i^{th}$ flip. Then we know that:
\begin{equation*}
    P(E_i) = (1-p)^{i-1}p \quad i = 1,2,...
\end{equation*}

You can compute the probability that Ann wins as:
\begin{equation*}
\begin{split}
    P(\text{Ann Wins}) &= \sum\limits_{\text{i odd}} (1-p)^{i-1}p\\
        &= \sum\limits_{k=0}^\infty (1-p)^{2k+1-1}p\\
        &= \frac{p}{1-(1-p)^2}
\end{split}
\end{equation*}

\\~\\

\noindent \textbf{Question 37b:} This question is somewhat confusing, but all you are really trying to calculate is: 
\begin{equation*}
\begin{split}
    P(C|W) &= \frac{P(W|C) \cdot P(C)}{P(W|A)\cdotP(A) + P(W|B)\cdot P(B) + P(W|C)\cdot P(C)}\\
        &= \frac{1 \cdot \frac{1}{3}}{\frac{1}{2}\cdot\frac{1}{3} + 0 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} } = \frac{2}{3}
\end{split}
\end{equation*}

A better question would be ``How should the warden have given his answer so that he didn't reveal any information?"

\\~\\

\noindent \textbf{Question 5:} If $P(A \cap B \cap C) = P(A)P(B)P(C)$, this does \textbf{not} imply that $A,B,C$ are independent! You need \textit{pairwise} independence, as well. (Mori drew a lovely picture in class of the intersections).

In any case... Here's the answer:

\begin{equation*}
\begin{split}
    P(A\cap B \cap C) &= P(A \cap B) = P(A|B) \cdot P(B)\\
                        &= \frac{1}{2} P(B)\\
                    P(B) &= P(B|C) \cdot P(C) + P(B|C^c) \cdot P(C^c) \\
                        &= \frac{1}{3}\cdot \frac{1}{90}\\
                    P(A\cap B \cap C) &= \frac{1}{2} \cdot \frac{1}{3}\cdot \frac{1}{90} = \frac{1}{540}
\end{split}
\end{equation*}


\section{Lecture - Part 1}
\subsection{Random Variables}

\textbf{Definition:} A \textit{random variable} is a function from the sample space to the real line $X: \mathcal{S} \to \mathbb{R}$\\~\\

\noindent \textbf{Examples:}
\begin{enumerate}
    \item We just talked about the example where we are sampling from $\{2,4,9,12\}$. There are many possibilities here, but we are not interested in those. We considered the random variable $X$, the mean of the four selected numbers.
    \begin{equation*}
    \begin{split}
        X(2, 2, 2, 2) &= 2\\
        X(4,4,9,9) &= 6.5
    \end{split}
    \end{equation*}
    
    When we choose 4 elements from the sample space, the random variable maps that event to the \textit{mean} of those numbers.
    
    \item If we take a vote and ask ``do you agree with President Trump's position on something?". Our random variable might be $X$, the proportion of ``Yes" answers. In this case, we have the random variable as follows:
    \begin{equation*}
        \begin{split}
            X(0,0,0,0,0)&= 0\\
            X(0,0,1,0,0) &= \frac{1}{5}
        \end{split}
    \end{equation*}
\end{enumerate}

\subsection{Section 1.5 - Distribution Functions}

Associated with every \textit{random variable}, $X$ is a function called the cumulative distribution function (CDF). The CDF is defined by:
\begin{equation*}
\begin{split}
    F_X(x) &= P(X \leq x)\\
    F_X : \mathbb{R} &\to [0,1]
\end{split}
\end{equation*}

\noindent\textbf{Example:} Assume that in a population 10\% smoke. We continuously and randomly select from this population (with replacement), until we reach a smoker. Let $X$ be the number of people picked. Give the CDF for $X$.

\begin{equation*}
    F_X(x) = P(X\leq x) = \sum\limits_{k=1}^x (0.9)^{k-1}(0.1) = 1-0.9^x
\end{equation*}

In general, we want to define $F_X(x)$ over the entire number line $(-\infty, \infty)$:
\begin{equation*}
F_X(x) = \begin{cases}
1-0.9^i & \text{if } i \leq x < i+1 \quad i = 1,2,...\\
0 & x < 1
\end{cases}
\end{equation*}

\section{Lecture - Part 2}

\subsection{Properties of CDF}
\textbf{Theorem:} The function $F(x)$ is a CDF \textbf{iff} the following conditions hold:
\begin{enumerate}
    \item $F$ is non-decreasing
    \item $\lim_{x\to\infty} F(x) = 1$ and $\lim_{x\to-\infty} F(x) = 0$
    \item $F$ is \textbf{right-continuous}. That is, the limit as $x$ goes to $y$ from the right implies $F(x) = F(y)$ 
\end{enumerate}

\textbf{Proof:}
\begin{enumerate}
    \item Here, $F$ is just a real-valued function. We need to show that for all $x<y$, we have $f(x) \leq f(y)$ (\textit{non-decreasing}). 
    
    Suppose that $x<y$:
    \begin{equation*}
    \begin{split}
        F_X(x) &= P(X\leq x)\\
        F_X(y) &= P(X \leq y)
    \end{split}
    \end{equation*}
    
    Since $x<y$, we have the set $\{X \leq x\} \subset \{X \leq y\}$, and we are done.
    
    \item We need to show that $\lim_{x\to\infty}F(x) = 1$. We will pick a sequence of real numbers that converge to $\infty$. Let $x_1 \leq x_2 \leq x_3 \leq ...$ be a sequence of real numbers that approach infinity. Consider the following events:
    \begin{equation*}
        A_1 = \{X \leq x_1\}, A_2 = \{X \leq x_2\},...
    \end{equation*}
    
    Thus $A_1 \subset A_2 \subset A_3 ...$ is an increasing set of events. It is sufficient to show that $\lim_{n\to\infty} F(x_n) = 1$ for any sequence that approaches infinity.
    
    \begin{equation*}
        \begin{split}
            \lim_{n\to\infty} F(x_n) &= \lim_{n\to\infty} P(X \leq x_n) \\
            &= \lim_{n\to\infty} P(A_n) 
        \end{split}
    \end{equation*}
    
    Since $P$ is a \textit{continuous set function}, we have:
    \begin{equation*}
        \lim_{n\to\infty} P(A_n) = P \left[ \lim_{n\to\infty} A_n \right]
    \end{equation*}
    
    Since $A_n = \{X \leq x_n\} \rightarrow \{x \leq \infty\}$ (as $n\to\infty$), this implies that the limit is $P(x< \infty) = 1$.
    \item For part 3, you pick a sequence of $x_n$ which approach $y$ from the right. This, as well as the second half of (2), is left as an exercise for the reader.
\end{enumerate}

\subsection{Continuous Random Variables}

\textbf{Definition:} A random variable is \textbf{continuous} if $F_X(x)$ is \textit{continuous} for all $x \in \mathbb{R}$. A random variable is \textbf{discrete} if $F_X(x)$ is a \textit{step function}.

Examples: 
\begin{itemize}
    \item (Discrete RV): The smoker problem from earlier
    \item (Continuous RV): Consider the CDF
    \begin{equation*}
      F_X(x) = \begin{cases}
            1 - e^{-x} & x > 0\\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    
    \item Mixed Continuous-Discrete (the graph jumps)
    \begin{equation*}
      F_X(x) = \begin{cases}
            \frac{1}{2}x & 0 \leq x < 1\\
            \frac{x + 2}{x + 3} & x\geq 1 \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{itemize}

\noindent\textbf{Theorem:} Let $X$ be a random variable with CDF $F_X$. Then for $a<b$ (scalars)
\begin{equation*}
    P\left[a< X \leq b\right] = F(b) - F(a)
\end{equation*}

\noindent\textbf{Proof:} We can write $\{X \leq b\} = \{X \leq a\} \cup \{a < X \leq b\}$. Thus, since these two sets are disjoint, we have:
\begin{equation*}
    P(X\leq b) = P(X \leq a) + P(a < X \leq b)
\end{equation*}

And we are done.
\\~\\
\noindent \textbf{Theorem:} For any random variable
\begin{equation*}
    P(X=x) = F_X(x) - F_X(x^-), \quad \forall x \in \mathbb{R}
\end{equation*}

Where $F_X(x^-) = \lim_{z\uparrow x}F_X(z)$.

\noindent \textbf{Proof:} For any $x \in \mathbb{R}$
\begin{equation*}
    \{x\} = \bigcap\limits_{n=1}^\infty \left(x-\frac{1}{n}, x\right]
\end{equation*}

Let 
\begin{equation*}
\begin{split}
    A_n &= \{x \in (x - \frac{1}{n}, x \}\\
    A_1 &\supset A_2 \supset ...
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
    \lim_{n\to\infty} A_n &= \bigcap\limits_{i=1}^\infty A_i = \{x\}\\
    P(X = x) &= P\left[\bigcap\limits_{i=1}^\infty A_i \right]= P\left[\lim_{n\to\infty} A_n \right]\\
    &= \lim_{n\to\infty} P(A_n)\\
    &= \lim_{n\to\infty} P(x-\frac{1}{n} < X \leq x)\\
    &= \lim_{n\to\infty} \left[F_X(x) - F_X(x- \frac{1}{n}) \right]\\
    &= F_X(x) - F_X(x^-)
\end{split}
\end{equation*}


\noindent \textbf{Theorem:} If $X$ is a continuous random variable, then $\forall x \in \mathbb{R}$
\begin{equation*}
    P(X=x) = 0
\end{equation*}

\noindent \textbf{Proof:} $P(X = x) = F_X(x) - F_X(x^-)$. Since $F_X(x^-) = F_X(x)$ when $F_X$ is a continuous function, we are done. 

\\~\\

\noindent \textbf{Definition:} Let $B^1$ be the smallest $\sigma$-algebra containing all intervals of the form $(a,b), [a,b], (a,b], [a,b)$.

We say that two random variables $X$ and $Y$ are \textbf{identically distributed} if for every $A \in B^{1}$, we have $P(X \in A) = P(Y \in A)$.

\\~\\

\noindent \textbf{Theorem:} $X$ and $Y$ are identically distributed \textbf{iff} $F_X(x) = F_Y(x)$ for all $x$.

\noindent \textbf{Proof:} ($\Rightarrow$) If $X$ and $Y$ are identically distributed, then for every $A \in B^1$, then
\begin{equation*}
    P(X \in A) = P(Y \in A)
\end{equation*}

Let $A = (-\infty, X] \in B^1$. Then:
\begin{equation*}
    \begin{split}
        P(X \in (-\infty,X]) &= P(Y \in (-\infty,X])\\
        \Rightarrow F_X(x) &= F_Y(y)
    \end{split}
\end{equation*}

\end{document}